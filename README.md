# Navierâ€“Stokes â€” Physics-Informed & Multi-Fidelity Neural Networks

This repository explores **data-driven modeling for Navierâ€“Stokes / CFD** using a **Multi-Fidelity Neural Network (MFNN)**.

The goal is to combine **cheap, low-fidelity CFD data** with **expensive, high-fidelity data** to learn an accurate surrogate model â€” improving prediction quality while reducing simulation cost.

## ğŸ” Motivation

Classical CFD solvers are accurate but computationally expensive.  
Low-fidelity solvers are faster, but introduce bias.

ğŸ‘‰ A **Multi-Fidelity Neural Network** leverages *both*:

- **Low-fidelity data** â†’ captures global structure  
- **High-fidelity data** â†’ corrects local errors  

This approach is highly relevant for **surrogate modeling, optimization, digital twins, and engineering design**.

## ğŸ¯ Objective

We learn two mappings from inputs \(x\):

$$
x \mapsto \hat{y}_{\mathrm{low}}(x)
$$

$$
x \mapsto \hat{y}_{\mathrm{high}}(x)
$$

The high-fidelity network **reuses information** from the low-fidelity model.

## ğŸ§  Model Architecture

The MFNN contains three main components:

### 1ï¸âƒ£ Shared Feature Extractor
Transforms the input into a latent space:
$$
z(x)=f_{\theta}(x)
$$


### 2ï¸âƒ£ Low-Fidelity Head
Predicts the coarse approximation:Ã¹

$$
\hat{y}_{\mathrm{low}} = g_{\theta_{\mathrm{low}}}\!\left(z\right)
$$

### 3ï¸âƒ£ High-Fidelity Correction Head
Refines the prediction using both the latent space and the LF estimate:

$$
\hat{y}_{\mathrm{high}} = h_{\theta_{\mathrm{high}}}\!\left(z,\,\hat{y}_{\mathrm{low}}\right)
$$

This sharing mechanism is what makes the network *multi-fidelity*.

## ğŸ“‰ Loss Function

The network is trained using a weighted MSE loss:

$$
\mathcal{L} = \lambda_{\mathrm{low}}\, \mathrm{MSE}\!\left(\hat{y}_{\mathrm{low}},\, y_{\mathrm{low}}\right) + \lambda_{\mathrm{high}}\,  \mathrm{MSE}\!\left(\hat{y}_{\mathrm{high}},\,y_{\mathrm{high}}\right)
$$

Where:

- \( \lambda_{\text{low}} \) â€” weight of LF data  
- \( \lambda_{\text{high}} \) â€” weight of HF data  

Training proceeds such that the model first learns a **good low-fidelity approximation**, and then **refines it** with high-fidelity supervision.

## ğŸ“‚ Dataset

Included files:

- `y_l.dat` â€” Low-fidelity data  
- `y_h.dat` â€” High-fidelity data  
- `y_test.dat` â€” Test dataset  
- `mfdata.mat`, `mfdata2.mat` â€” MATLAB multi-fidelity datasets  

Typical workflow:

1. Load datasets  
2. (Optional) Normalize inputs/outputs  
3. Train MFNN  
4. Evaluate against test data  

---

## ğŸ“Š Visualizations

The repository includes figures comparing:

âœ” Low-fidelity vs. high-fidelity  
âœ” MFNN predictions vs. ground truth  
âœ” Error distributions  
âœ” Training evolution  

These illustrate how the network progressively improves from:

**Low-Fidelity â†’ High-Fidelity â†’ Multi-Fidelity**

> If you run the notebooks/scripts, figures are automatically generated.

---

## ğŸ“ Repository Structure

